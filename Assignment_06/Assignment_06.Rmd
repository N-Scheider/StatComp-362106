---
title: "Assignment 06"
author: "Noah Scheider"
date: "11/18/2022"
output: bookdown::html_document2
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

```{r data, include=FALSE}
library(VGAM)
```

# Exercise

We currently have 3 ways of generating Gaussian samples in R:

* the native `rnorm()` using the inverse transform 
* Box-Muller transform
* Rejection-Sampling (see lecture notes for code)

For 50 simulation runs, and for all three methods:

* generate $10^5$ samples using
* time the generation process using `Sys.time()`
* calculate the KS statistic ([the one-sample Kolmogorov-Smirnov statistic](https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test)) to check quality of the sample, e.g.
  
  * `X <- rnorm(10Ë†5)`
  * `F_hat <- ecdf(X)`
  * `F_hat_eval <- F_hat(sort(X))`
  * `F_eval <- pnorm(sort(X))`
  * `KS_stats <- max(abs(F_hat_eval - F_eval))`

Compare the three methods using your simulation results.

```{r echo=FALSE}
N <-  1e5
sim <- 50

t_inv <- rep(0, sim)
t_box <- rep(0, sim)
t_rej <- rep(0, sim)
KS_stats_inv <- rep(0, sim)
KS_stats_box <- rep(0, sim)
KS_stats_rej  <- rep(0, sim)

for (i in 1:sim) {

  # Gaussian Samples via inverse transform
  s0 <- Sys.time()
  X <- rnorm(N)
  F_inv_ecdf <- ecdf(X)
  F_hat_inv_eval <- F_inv_ecdf(sort(X))
  F_inv_eval <- pnorm(sort(X))
  KS_stats_inv[i] <- max(abs(F_hat_inv_eval - F_inv_eval))
  t_inv[i] <- Sys.time() - s0
  
  # Gaussian Samples via Box-Muller transform
  s2 <- Sys.time()
  U1 <- runif(N)
  U2 <- runif(N)
  X_box <- sqrt(-2*log(U1))*cos(2*pi*U2)
  F_box_ecdf <- ecdf(X_box)
  F_hat_box_eval <- F_box_ecdf(sort(X_box))
  F_box_eval <- pnorm(sort(X_box))
  KS_stats_box[i] <- max(abs(F_hat_box_eval - F_box_eval))
  t_box[i] <- Sys.time() - s2
  
  # Gaussian Samples via rejection sampling
  # c <- sqrt(2/pi)*exp(1/2)
  # xgrid <- seq(-3, 3, 0.01)
  # plot(xgrid, dnorm(xgrid), ylim = c(0, 0.7), type = 'l')
  # lines(xgrid, c*dlaplace(xgrid), col = "blue")
  # legend("topleft", legend = c("proposal", "target"), col = c("green", "black"), pch = 16)
  s3 <- Sys.time()
  X_rej <- rep(0,N)
  c <- sqrt(2/pi)*exp(1/2)
  count <- 0
  while(count != N){
    Y <- rlaplace(1)
    Ue <- runif(1)
    if (Ue<1/c*dnorm(Y)/dlaplace(Y)){
      X_rej[count+1] <- Y
      count <- count + 1
    }
  }
  F_rej_ecdf <- ecdf(X_rej)
  F_hat_rej_eval <- F_rej_ecdf(sort(X_rej))
  F_rej_eval <- pnorm(sort(X_rej))
  KS_stats_rej[i] <- max(abs(F_hat_rej_eval - F_rej_eval))
  t_rej[i] <- Sys.time() - s3
}

```

```{r fig.cap="The figures shows the amount of time the different sampling method need in order to generate 10^5 samples during multiple simulations", fig.align='center', echo=FALSE}

leg <- c("Inverse", "Box-Muller", "Rejection")

par(mfrow=c(1, 2))

plot(1:sim, t_inv, ylim = c(0, max(t_rej)), xlab = "simulation", ylab = "time", type = "l", col = 1)
lines(1:sim, t_box, col = 2)
lines(1:sim, t_rej, col = 3)
legend("topright", legend = leg, col = c(1:3), pch = 15)

plot(1:sim, t_inv, type = "l", col = 1, xlab = "simulation", ylab = "time", ylim = c(0.03, 0.036))
lines(1:sim, t_box, col = 2)
legend("topright", legend = leg[-3], col = c(1:2), pch = 15)

```

```{r include=FALSE, echo=FALSE}
dev.off()
```

We observe that the Rejection-Sampling method needs distinguishly more time throughout all the simulations where as Box-Muller and the Inverse method go toe-to-toe. Looking sharply at the results it seems that the Inverse method is slightly faster. Therefore we should pick the Inverse or the Box-Muller method when trying to generate standard normally distributed random variables with respect to computational efficiency. The cause of extended generating time of the Rejection-Sampling method may be due to a non-optimal choice of the proposal function enveloping the target function. In our case we went for a Laplacian distribution with the standard normal parameters. As a scaling constant we analytically proved the optimal value $c = \sqrt{\frac{2}{\pi}} \cdot e^{\frac{1}{2}}$. The scaled proposal function in comparision with its target function looks as follows:

```{r fig.cap="The Laplacian pdf with standard normal parameters was choosen in order to envlope the standard normal pdf after being scaled", fig.align='center', echo=FALSE}

c <- sqrt(2/pi)*exp(1/2)
xgrid <- seq(-5, 5, 0.01)
plot(xgrid, dnorm(xgrid), ylim = c(0, 0.7), type = 'l', xlab = "x", ylab = "density at x")
lines(xgrid, c*dlaplace(xgrid), col = "green")
legend("topright", legend = c("proposal", "target"), col = c("green", "black"), pch = 16)

```

We see that our proposal function mimics the behavior of the target function quite well. Nevertheless we observe that the shape of the proposal and the target function differ considerably in their global maximum at 0.

```{r fig.cap="The figures shows the the Kolmogorov-Smirnov Statistic of different sampling methods in 50 different simulations", fig.align='center', echo=FALSE}

plot(1:sim, KS_stats_inv, ylim = c(0.001, 0.006), , xlab = "simulation", ylab = "Kolmogorov-Smirnov statistic", type = "l", col = 1)
lines(1:sim, KS_stats_box, col = 2)
lines(1:sim, KS_stats_rej, col = 3)
legend("topright", legend = leg, col = c(1:3), pch = 15)

```

We observe strong similarity between the Kolmogorov-Smirnov statistics. This allows us to conclude equal "quality" of the generated random variables of the different methods. Overall, we can now decide in favour of the Inverse method when sampling standard normal Gaussian random variables as it seems to be the best algorithm, compared to the Box-Muller Method and the Rejection-Sampling method, with respect to its computational speed and the quality of its output.
