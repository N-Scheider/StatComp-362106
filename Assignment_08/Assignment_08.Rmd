---
title: "Assignment 08"
author: "Noah Scheider"
date: "2022-12-09"
output: bookdown::html_document2
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo=TRUE, warning=FALSE, message=FALSE)
```

# Exercise

Since black carbon (BC) is a pollutant known for its adverse health effects,
it is of interest to monitor BC mass concentration in urban areas.
While stationary measurement devices are able to precisely 
record BC concentrations (random variable $X$), 
simpler mobile devices provide more flexibility -- at the cost of
some measurement noise $\varepsilon$.
For a mobile measurement, only $Y = X + \varepsilon$ is observed, where we may assume $\varepsilon \sim N(0, \sigma^2)$ with known standard deviation $\sigma = 0.6\, \mu g m^{-3}$ based on lab experiments.

To obtain a detailed overview over the BC concentrations in Lausanne,
the aim is to add mobile measurements to those of available stationary devices. We want to predict $X$ given a noisy observation $Y=y$ using a
Bayesian approach, with stationary measurements motivating a priori a Weibull distribution for $X$ with shape parameter $2$ and scale parameter $1.2$ (median $\approx 1 \, \mu g m^3$).

- Implement a Metropolis-Hastings-Algorithm for obtaining MCMC samples of $X \mid Y=y$ using $N(y, 0.6)$ as *fixed proposal distribution for all iterations* (note that this is an asymmetric proposal from the MH perspective!).

- Run your algorithm for $y = 0.5$, $y = 1$ and $y = 2$ for illustration. 
  In each run, draw $10000$ samples after a burn-in of $1000$ 
  (less if it takes too long).
  
- In this specific scenario, the posterior is in fact analytically available 
  with the R code for the density function provided on the next slide.
  Graphically compare the empirical distributions of your MCMC samples with
  the true posterior densities and the proposal densities for the three considered values of $y$.

*Hint:* It might be wise to compute $\log(\alpha)$ first in dependence on $\log$-densities to avoid numerical issues.


```{r echo=FALSE}
# Imported from Assignment
dposterior <- function(x, y, scale = 1.2, sd = .6) {
  # x: evaluation points of the density
  # y: observation Y=y (length 1),
  # scale: scale parameter of Weibull prior (shape=2 fixed)
  # sd: standard deviation of Gaussian error (mean=0 fixed) 
  a <- 1/2*1/sd^2
  c <- 1/scale^2
  erf <- function(x) 2*pnorm(x*sqrt(2)) - 1
  k <- ifelse(x >= 0, x * exp( -a * (x-y)^2 - c*x^2 ), 0) 
  n <- exp(-a*(y^2))*(sqrt(pi)*a*y*exp(a^2*y^2/(a+c))*(erf(a*y/sqrt(a+c))+1)+sqrt(a+c))/(2*(a+c)^(3/2))
  return(k/n)
}

MetropolisHasting <- function(N, y){
  MC <- rep(0, N+1)
  acc <- rep(0, N)
  MC[1] <- y
  for (n in 1:N) {
    X <- rnorm(1, mean=y, sd=0.6)
    acc[n] <- min(1, dweibull(X, shape=2, scale=1.2)/dweibull(MC[n], shape=2, scale=1.2)*dnorm(MC[n], mean=y, sd=0.6)/dnorm(X, mean=y, sd=0.6))
    u <- runif(1, min=0, max=1)
    MC[n+1] <- X*(u<=acc[n]) + MC[n]*(u>acc[n])
  }
  return(list(MC, acc))
}
```

To get a better picture of what we are dealing with and what exactly we are going to compare in the histogram plots, we will plot trajectories of several Markov Chain with size 100 (for visualization purposes only). Those chains are sampled by a Metropolis-Hasting Algorithm with features described in the assignment. Furthermore, the MC's are conditioned on different observations of $y$.

```{r fig.cap="Trace-plots of the Markov Chains conditioned on different observations Y=0.5,1,2", fig.align='center', echo=FALSE}

set.seed(2001)
N <- 100

MC_a <- MetropolisHasting(N, 0.5)[[1]]
MC_b <- MetropolisHasting(N, 1)[[1]]
MC_c <- MetropolisHasting(N, 2)[[1]]

plot(1:(N+1), MC_a, "l", col=1, xlab="time", ylab="state")
lines(1:(N+1), MC_b, col=2)
lines(1:(N+1), MC_c, col=3)
title(main="MC with Metropolis Hasting")
legend("topright", legend=c("y=0.5", "y=1", "y=2"), col=c(1,2,3), pch=16)

```
We see that the Markov chains conditioned on $y=0.5$ and especially the one conditioned $y=2$ have noticeable rejection phases, where the Algorithm rejects the proposal given by $X|Y=y \sim \mathcal{N}(y, 0.6)$ consecutively. Lets see how this translates into the according histograms.


```{r fig.cap="Histogram of Markov Chain Sampled by a Metropolis Hasting Algorithm with proposal and target function described in the exercise with a posteriory (black) and proposal (red) densities", fig.align='center', echo=FALSE}

N <- 1000
burn_in <- 1000

MC_a <- MetropolisHasting(burn_in+N, 0.5)
acc_a <- mean(MC_a[[2]][-c(1:burn_in)])
MC_a <- MC_a[[1]][-c(1:burn_in)]
MC_b <- MetropolisHasting(burn_in+N, 1)
acc_b <- mean(MC_b[[2]][-c(1:burn_in)])
MC_b <- MC_b[[1]][-c(1:burn_in)]
MC_c <- MetropolisHasting(burn_in+N, 2)
acc_c <- mean(MC_c[[2]][-c(1:burn_in)])
MC_c <- MC_c[[1]][-c(1:burn_in)]

par(mfrow=c(1, 3))
# given observation y=0.5
hist(MC_a, ylim=c(0,1.3), probability=T, xlab="state, y=0.5", main="", breaks = seq(min(MC_a), max(MC_a), length.out = 25))
lines(seq(min(MC_a), max(MC_a), 0.1), dposterior(seq(min(MC_a), max(MC_a), 0.1), 0.5), col=1)
lines(seq(min(MC_a), max(MC_a), 0.1), dnorm(seq(min(MC_a), max(MC_a), 0.1), 0.5, 0.6), col=2)
# given observation y=1
hist(MC_b, ylim=c(0,1.3), probability=T, xlab="state, y=1", main="", breaks = seq(min(MC_b), max(MC_b), length.out = 25))
lines(seq(min(MC_b), max(MC_b), 0.1), dposterior(seq(min(MC_b), max(MC_b), 0.1), 1), col=1)
lines(seq(min(MC_b), max(MC_b), 0.1), dnorm(seq(min(MC_b), max(MC_b), 0.1), 1, 0.6), col=2)
# given observation y=2
hist(MC_c, ylim=c(0,1.3), probability=T, xlab="state, y=2", main="", breaks = seq(min(MC_c), max(MC_c), length.out = 25))
lines(seq(min(MC_c), max(MC_c), 0.1), dposterior(seq(min(MC_c), max(MC_c), 0.1), 2), col=1)
lines(seq(min(MC_c), max(MC_c), 0.1), dnorm(seq(min(MC_c), max(MC_c), 0.1), 2, 0.6), col=2)
title("Histogram, Posteriori and Proposal densities", line = - 2, outer = TRUE)

```

In this plot we are able to compare the frequency of a MC being in a certain state to an analytically given posteriori density and the proposal density of the Metropolis-Hasting Algorithm. The posteriori density is plotted in black, whereas the proposal density is plotted in red. The densities are thus evaluated on the range of the states of its according Markov Chain after a burn-in of the first $1000$ samples for every chain.

We can definitely conclude that there is a strong connection. Looking sharply at the results we can even assume that the the densities given $y=1$ actually approximates the histogram of the Markov Chain conditioned on $y=1$ aswell better then compared to the other ones. By analyzing the previous trace-plots of the different Markov Chains we found out, that the Chains given $y=0.5,2$ reject far more often then for $y=1$. Hence, there seems to be a link between the acceptance rate of the Metropolis-Hasting Algorithm and the approximation by a known posteriori density. Furthermore, the densities seem to be on the side of the histogram with the the less informations or equivalently the visited states. In the following we plot the different acceptance rates for the different $y$ that will underline our assumption.

```{r echo=FALSE}
print(paste0("Acceptance rate for y=0.5: ", acc_a))
print(paste0("Acceptance rate for y=1: ", acc_b))
print(paste0("Acceptance rate for y=2: ", acc_c))
```

Another subtlety is the fact that the last histogram hints a second peek near to $0$, which does not match with neither of the posteriori or the proposal density. Again, this might be due to an unfavorable generation of proposals which were then rejected very often in a row.
